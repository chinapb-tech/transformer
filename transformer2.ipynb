{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## WordEmbedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.2594,  0.4778,  0.8906,  0.6225, -0.6952, -0.0059],\n",
       "         [ 0.4469, -0.0968, -0.6293,  0.4834,  0.9538,  1.4804],\n",
       "         [-1.2663, -1.6941, -1.0667,  0.3993,  0.4741,  0.7654],\n",
       "         [ 0.7726,  0.0513,  0.2658,  0.2169,  0.0769, -0.4826],\n",
       "         [-0.5204, -0.7254, -1.4262,  0.1011,  0.0400,  0.0062],\n",
       "         [-0.0191, -1.0718,  0.6736, -0.9083,  0.7761, -0.7482],\n",
       "         [ 0.2084, -0.9778, -0.6865, -0.3744, -0.1471, -1.9092],\n",
       "         [ 0.3867, -0.1872, -0.3631, -0.3801,  0.8723,  0.6902]],\n",
       "\n",
       "        [[-0.2594,  0.4778,  0.8906,  0.6225, -0.6952, -0.0059],\n",
       "         [ 0.2681,  0.4924, -0.3625,  0.9996, -0.7339,  0.2405],\n",
       "         [-0.0191, -1.0718,  0.6736, -0.9083,  0.7761, -0.7482],\n",
       "         [-1.2663, -1.6941, -1.0667,  0.3993,  0.4741,  0.7654],\n",
       "         [ 0.5060, -0.6744,  0.5305, -0.6060,  0.8598, -1.4691],\n",
       "         [ 0.2084, -0.9778, -0.6865, -0.3744, -0.1471, -1.9092],\n",
       "         [ 0.3867, -0.1872, -0.3631, -0.3801,  0.8723,  0.6902],\n",
       "         [ 0.3867, -0.1872, -0.3631, -0.3801,  0.8723,  0.6902]],\n",
       "\n",
       "        [[-0.2594,  0.4778,  0.8906,  0.6225, -0.6952, -0.0059],\n",
       "         [-0.6043,  0.3150, -1.7352,  0.1188, -1.5499,  0.3477],\n",
       "         [-0.0191, -1.0718,  0.6736, -0.9083,  0.7761, -0.7482],\n",
       "         [-1.2311,  1.1654, -0.6477,  2.4295, -1.1219,  0.9785],\n",
       "         [ 0.5060, -0.6744,  0.5305, -0.6060,  0.8598, -1.4691],\n",
       "         [-0.5204, -0.7254, -1.4262,  0.1011,  0.0400,  0.0062],\n",
       "         [ 0.2681,  0.4924, -0.3625,  0.9996, -0.7339,  0.2405],\n",
       "         [ 0.2084, -0.9778, -0.6865, -0.3744, -0.1471, -1.9092]]],\n",
       "       grad_fn=<EmbeddingBackward0>)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "class WordEmbedding(nn.Module):\n",
    "    def __init__(self, vocab_size: int, d_model: int):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, d_model)\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        \"\"\"x.shape=(batch_size,seq_len)\"\"\"\n",
    "        return self.embedding(x)\n",
    "\n",
    "\n",
    "batch_sentences = torch.tensor(\n",
    "    [\n",
    "        [101, 3, 2, 5, 7, 8, 102, 0],\n",
    "        [101, 13, 8, 2, 9, 102, 0, 0],\n",
    "        [101, 21, 8, 15, 9, 7, 13, 102],\n",
    "    ]\n",
    ")\n",
    "\n",
    "vocab_size = 200\n",
    "d_model = 6\n",
    "word_embedding_cls = WordEmbedding(vocab_size=vocab_size, d_model=d_model)\n",
    "word_embedding = word_embedding_cls(batch_sentences)\n",
    "word_embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PositionalEmbedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "class PositionalEmbedding(nn.Module):\n",
    "    def __init__(self, max_len: int, d_model: int):\n",
    "        super().__init__()\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len)\n",
    "        div_term = torch.exp(-torch.arange(0, d_model, 2) * math.log(10000.0) / d_model)\n",
    "        freq = torch.outer(position, div_term)\n",
    "        pe[:, 0::2] = torch.sin(freq)\n",
    "        pe[:, 1::2] = torch.cos(freq)\n",
    "        pe = pe.unsqueeze(0)\n",
    "        self.register_buffer(\"pe\", pe)\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        \"\"\"x.shape=(batch_size,seq_len,d_model)\"\"\"\n",
    "        return x + self.pe[:, : x.shape[1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-1.9628,  1.0211, -1.3259,  0.5956,  0.5534,  0.9839],\n",
       "         [-0.2777,  2.4332,  1.5308,  0.9200, -2.2194,  1.3981],\n",
       "         [ 1.0854, -1.5181, -0.1493,  1.3841,  1.2800,  2.0469],\n",
       "         [-0.6702, -1.9092, -1.5013,  1.0365, -1.3446,  0.6457],\n",
       "         [-1.0493, -1.3474, -0.4281, -0.1104, -0.3846,  2.6994],\n",
       "         [-0.2033, -0.1855,  0.3566,  0.0544, -0.2543,  2.5764],\n",
       "         [ 0.9776,  1.3213, -0.5522,  0.2842, -0.9604,  1.7178],\n",
       "         [ 1.4179,  0.6697,  0.1250,  0.6449, -0.5645, -1.6632]],\n",
       "\n",
       "        [[-1.9628,  1.0211, -1.3259,  0.5956,  0.5534,  0.9839],\n",
       "         [ 1.6129,  1.6560,  0.4810,  1.9849, -0.5798,  1.1292],\n",
       "         [ 1.6649, -0.8853,  0.2193,  0.0769, -0.2608,  2.5764],\n",
       "         [ 0.3172, -2.0919, -0.1032,  1.3787,  1.2821,  2.0469],\n",
       "         [-1.7056,  0.1021,  2.1960,  1.7241, -1.0572,  2.1324],\n",
       "         [ 0.2981,  0.6448, -0.5971,  0.2959, -0.9625,  1.7178],\n",
       "         [ 0.4815,  0.8760,  0.0807,  0.6587, -0.5666, -1.6631],\n",
       "         [ 1.4179,  0.6697,  0.1250,  0.6449, -0.5645, -1.6632]],\n",
       "\n",
       "        [[-1.9628,  1.0211, -1.3259,  0.5956,  0.5534,  0.9839],\n",
       "         [ 0.6147, -1.9975,  2.2083,  1.9887, -0.4469,  0.7145],\n",
       "         [ 1.6649, -0.8853,  0.2193,  0.0769, -0.2608,  2.5764],\n",
       "         [ 2.0244,  0.0397,  0.6077,  4.0091, -1.2849,  0.4995],\n",
       "         [-1.7056,  0.1021,  2.1960,  1.7241, -1.0572,  2.1324],\n",
       "         [-1.2515, -0.4100, -0.3827, -0.1200, -0.3824,  2.6993],\n",
       "         [ 0.4920,  2.0759,  0.7095,  1.9474, -0.5690,  1.1291],\n",
       "         [ 1.9140,  1.1151, -0.5078,  0.2704, -0.9582,  1.7178]]],\n",
       "       grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_sentences = torch.tensor(\n",
    "    [\n",
    "        [101, 3, 2, 5, 7, 8, 102, 0],\n",
    "        [101, 13, 8, 2, 9, 102, 0, 0],\n",
    "        [101, 21, 8, 15, 9, 7, 13, 102],\n",
    "    ]\n",
    ")\n",
    "\n",
    "max_len = 512\n",
    "vocab_size = 200\n",
    "d_model = 6\n",
    "word_embedding_cls = WordEmbedding(vocab_size=vocab_size, d_model=d_model)\n",
    "word_embedding = word_embedding_cls(batch_sentences)\n",
    "positional_embedding_cls = PositionalEmbedding(max_len, d_model)\n",
    "positional_embedding = positional_embedding_cls(word_embedding)\n",
    "positional_embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 合并embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "class Embedding(nn.Module):\n",
    "    def __init__(self, vocab_size: int, max_len: int, d_model: int):\n",
    "        super().__init__()\n",
    "        self.word_embedding = WordEmbedding(vocab_size, d_model)\n",
    "        self.pos_embedding = PositionalEmbedding(max_len, d_model)\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        \"\"\"x.shape=(batch_size,seq_len)\"\"\"\n",
    "        return self.pos_embedding(self.word_embedding(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.4572, -0.2178,  0.0781,  1.8263, -0.5870,  2.0135],\n",
       "         [ 1.6376, -0.4071, -1.5502,  0.9801,  2.0680,  0.6822],\n",
       "         [ 1.1654,  0.3321, -1.0168, -0.1903, -1.4823,  1.2675],\n",
       "         [ 1.0693, -0.2825,  0.6275,  0.8787,  1.8801,  0.6586],\n",
       "         [-0.2748,  0.4763, -1.2725,  0.7864,  0.8565,  0.7479],\n",
       "         [-1.1064, -0.9123,  0.5808, -1.3677, -0.0208,  1.3122],\n",
       "         [ 0.3025,  1.2600, -0.3686,  0.1991,  2.8254,  1.5712],\n",
       "         [ 1.0995,  1.8471,  0.9251,  1.8300,  0.0190,  1.8744]],\n",
       "\n",
       "        [[-0.4572, -0.2178,  0.0781,  1.8263, -0.5870,  2.0135],\n",
       "         [-0.0238,  0.1447, -0.3907,  0.5942,  0.3845,  0.1024],\n",
       "         [ 0.7618, -1.6121,  0.4435, -1.3452, -0.0273,  1.3123],\n",
       "         [ 0.3973, -0.2417, -0.9707, -0.1957, -1.4802,  1.2675],\n",
       "         [-1.8862,  0.0429, -0.4639,  0.2320, -0.4949,  0.1739],\n",
       "         [-0.3770,  0.5835, -0.4135,  0.2108,  2.8233,  1.5712],\n",
       "         [ 0.1631,  2.0533,  0.8808,  1.8438,  0.0168,  1.8745],\n",
       "         [ 1.0995,  1.8471,  0.9251,  1.8300,  0.0190,  1.8744]],\n",
       "\n",
       "        [[-0.4572, -0.2178,  0.0781,  1.8263, -0.5870,  2.0135],\n",
       "         [ 1.2348,  2.0664,  0.7026,  0.7047,  1.7512,  1.4597],\n",
       "         [ 0.7618, -1.6121,  0.4435, -1.3452, -0.0273,  1.3123],\n",
       "         [ 0.6888, -0.3181,  1.9815,  1.1123,  0.2493,  1.2197],\n",
       "         [-1.8862,  0.0429, -0.4639,  0.2320, -0.4949,  0.1739],\n",
       "         [-0.4769,  1.4136, -1.2271,  0.7767,  0.8586,  0.7479],\n",
       "         [-1.1447,  0.5646, -0.1622,  0.5567,  0.3953,  0.1023],\n",
       "         [ 1.2389,  1.0538, -0.3243,  0.1853,  2.8276,  1.5711]]],\n",
       "       grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_sentences = torch.tensor(\n",
    "    [\n",
    "        [101, 3, 2, 5, 7, 8, 102, 0],\n",
    "        [101, 13, 8, 2, 9, 102, 0, 0],\n",
    "        [101, 21, 8, 15, 9, 7, 13, 102],\n",
    "    ]\n",
    ")\n",
    "\n",
    "max_len = 512\n",
    "vocab_size = 200\n",
    "d_model = 6\n",
    "embedding_cls = Embedding(vocab_size=vocab_size, max_len=max_len, d_model=d_model)\n",
    "embedding_results = embedding_cls(batch_sentences)\n",
    "embedding_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## padding mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "def padding_mask(x: torch.Tensor, pad_value: int = 0):\n",
    "    return x != pad_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ True,  True,  True,  True,  True,  True,  True, False],\n",
       "        [ True,  True,  True,  True,  True,  True, False, False],\n",
       "        [ True,  True,  True,  True,  True,  True,  True,  True]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_sentences = torch.tensor(\n",
    "    [\n",
    "        [101, 3, 2, 5, 7, 8, 102, 0],\n",
    "        [101, 13, 8, 2, 9, 102, 0, 0],\n",
    "        [101, 21, 8, 15, 9, 7, 13, 102],\n",
    "    ]\n",
    ")\n",
    "\n",
    "padding_mask(batch_sentences, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## sequence mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "def sequence_mask(size: int):\n",
    "    size = (1, size, size)\n",
    "    return torch.tril(torch.ones(size), diagonal=1) != 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ True,  True, False, False, False],\n",
       "         [ True,  True,  True, False, False],\n",
       "         [ True,  True,  True,  True, False],\n",
       "         [ True,  True,  True,  True,  True],\n",
       "         [ True,  True,  True,  True,  True]]])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sequence_mask(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 合并mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "class Mask:\n",
    "    def padding_mask(self, x: torch.Tensor, pad_value: int = 0):\n",
    "        return (x != pad_value).unsqueeze(-2)\n",
    "\n",
    "    def sequence_mask(self, size: int):\n",
    "        size = (1, size, size)\n",
    "        return torch.tril(torch.ones(size)) != 0\n",
    "\n",
    "    def merge_mask(self, x: torch.Tensor, size: int, pad_value: int = 0):\n",
    "        return self.padding_mask(x, pad_value) & self.sequence_mask(size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ True, False, False, False, False, False, False, False],\n",
       "         [ True,  True, False, False, False, False, False, False],\n",
       "         [ True,  True,  True, False, False, False, False, False],\n",
       "         [ True,  True,  True,  True, False, False, False, False],\n",
       "         [ True,  True,  True,  True,  True, False, False, False],\n",
       "         [ True,  True,  True,  True,  True,  True, False, False],\n",
       "         [ True,  True,  True,  True,  True,  True,  True, False],\n",
       "         [ True,  True,  True,  True,  True,  True,  True, False]],\n",
       "\n",
       "        [[ True, False, False, False, False, False, False, False],\n",
       "         [ True,  True, False, False, False, False, False, False],\n",
       "         [ True,  True,  True, False, False, False, False, False],\n",
       "         [ True,  True,  True,  True, False, False, False, False],\n",
       "         [ True,  True,  True,  True,  True, False, False, False],\n",
       "         [ True,  True,  True,  True,  True,  True, False, False],\n",
       "         [ True,  True,  True,  True,  True,  True, False, False],\n",
       "         [ True,  True,  True,  True,  True,  True, False, False]],\n",
       "\n",
       "        [[ True, False, False, False, False, False, False, False],\n",
       "         [ True,  True, False, False, False, False, False, False],\n",
       "         [ True,  True,  True, False, False, False, False, False],\n",
       "         [ True,  True,  True,  True, False, False, False, False],\n",
       "         [ True,  True,  True,  True,  True, False, False, False],\n",
       "         [ True,  True,  True,  True,  True,  True, False, False],\n",
       "         [ True,  True,  True,  True,  True,  True,  True, False],\n",
       "         [ True,  True,  True,  True,  True,  True,  True,  True]]])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_sentences = torch.tensor(\n",
    "    [\n",
    "        [101, 3, 2, 5, 7, 8, 102, 0],\n",
    "        [101, 13, 8, 2, 9, 102, 0, 0],\n",
    "        [101, 21, 8, 15, 9, 7, 13, 102],\n",
    "    ]\n",
    ")\n",
    "\n",
    "mask = Mask()\n",
    "mask.merge_mask(batch_sentences, batch_sentences.shape[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MultiHeadAtten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import copy\n",
    "\n",
    "\n",
    "def clone(layer, nums: int):\n",
    "    return nn.ModuleList([copy.deepcopy(layer) for _ in range(nums)])\n",
    "\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, heads: int, d_model: int):\n",
    "        super().__init__()\n",
    "        assert d_model % heads == 0\n",
    "        self.d_k = d_model // heads\n",
    "        self.heads = heads\n",
    "        self.d_model = d_model\n",
    "        self.linears = clone(nn.Linear(d_model, d_model), 4)\n",
    "\n",
    "    def forward(\n",
    "        self, query: torch.Tensor, key: torch.Tensor, value: torch.Tensor, mask: None\n",
    "    ):\n",
    "        \"\"\"\n",
    "        query,key,value.shape=(batch_size,seq_len,d_model)\n",
    "        如果是编码器或者解码器，则mask.shape=(batch_size,1,seq_len)或(batch_size,seq_len,seq_len)\n",
    "        如果是编码解码,这mask.shape=(batch_size,tgt_seq_len,src_seq_len)\n",
    "        \"\"\"\n",
    "        if mask is not None:\n",
    "            mask = mask.unsqueeze(1)\n",
    "\n",
    "        batch_size = query.shape[0]\n",
    "        query, key, value = [\n",
    "            func(data).reshape(batch_size, -1, self.heads, self.d_k).transpose(1, 2)\n",
    "            for func, data in zip(self.linears, (query, key, value))\n",
    "        ]\n",
    "        scores, p_atten = self._attention(query, key, value, mask)\n",
    "        x = scores.transpose(1, 2).reshape(batch_size, -1, self.d_k * self.heads)\n",
    "        return self.linears[-1](x)\n",
    "\n",
    "    def _attention(\n",
    "        self, query: torch.Tensor, key: torch.Tensor, value: torch.Tensor, mask: None\n",
    "    ):\n",
    "        \"\"\"\n",
    "        query,key,value.shape=(batch_size,heads,seq_len,d_k)\n",
    "        如果是编码器或者解码器，则mask.shape=(batch_size,1,1,seq_len)或(batch_size,1,seq_len,seq_len)\n",
    "        如果是编码解码,这mask.shape=(batch_size,1,tgt_seq_len,src_seq_len)\n",
    "\n",
    "        返回结果,如果是自注意力，则shape=(batch_size,heads,seq_len,seq_len)\n",
    "        如果编解码的注意力，则shape=(batch_size,heads,tgt_seq_len,src_seq_len)\n",
    "        \"\"\"\n",
    "        d_k = query.shape[-1]\n",
    "        scores = torch.matmul(query, key.transpose(-2, -1)) / math.sqrt(d_k)\n",
    "        if mask is not None:\n",
    "            scores.masked_fill(mask == 0, -1e9)\n",
    "        p_atten = torch.softmax(scores, dim=-1)\n",
    "        return torch.matmul(p_atten, value), p_atten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.1197,  0.2151, -0.0453, -0.4315,  0.3465,  0.2176],\n",
       "         [ 0.1192,  0.2337, -0.0570, -0.4271,  0.3186,  0.2096],\n",
       "         [ 0.1178,  0.2306, -0.1040, -0.4975,  0.2167,  0.1857],\n",
       "         [ 0.1357,  0.2449, -0.1105, -0.4672,  0.2813,  0.1615],\n",
       "         [ 0.2392,  0.2149, -0.1005, -0.4600,  0.2652,  0.1909],\n",
       "         [ 0.2547,  0.2442, -0.1612, -0.4851,  0.2334,  0.1312],\n",
       "         [-0.0236,  0.2142, -0.0539, -0.5058,  0.2773,  0.2103],\n",
       "         [ 0.1703,  0.2669, -0.1975, -0.5378,  0.1658,  0.0973]],\n",
       "\n",
       "        [[ 0.1032,  0.2625, -0.1649, -0.5457,  0.1226,  0.1454],\n",
       "         [ 0.3144,  0.3140, -0.2243, -0.4838,  0.1277,  0.0781],\n",
       "         [ 0.1356,  0.2988, -0.2279, -0.5573, -0.0030,  0.1383],\n",
       "         [ 0.0794,  0.3018, -0.2447, -0.5909, -0.0651,  0.1452],\n",
       "         [-0.2205,  0.2848, -0.2742, -0.7478, -0.2236,  0.1613],\n",
       "         [-0.0886,  0.2686, -0.2149, -0.6556, -0.0600,  0.1642],\n",
       "         [ 0.1743,  0.3550, -0.3417, -0.6023, -0.0432,  0.0146],\n",
       "         [ 0.1084,  0.3409, -0.3246, -0.6226, -0.0775,  0.0495]],\n",
       "\n",
       "        [[ 0.1117,  0.1716,  0.1113, -0.3663,  0.4709,  0.2829],\n",
       "         [ 0.1310,  0.1687,  0.1609, -0.3344,  0.4309,  0.3351],\n",
       "         [ 0.1946,  0.1865,  0.0243, -0.4212,  0.2600,  0.2828],\n",
       "         [-0.0432,  0.1815,  0.1812, -0.3127,  0.4965,  0.3637],\n",
       "         [-0.3909,  0.1873, -0.0540, -0.6884, -0.0790,  0.3336],\n",
       "         [ 0.3717,  0.1910,  0.0105, -0.3713,  0.4274,  0.1933],\n",
       "         [ 0.6103,  0.2478, -0.0738, -0.3095,  0.4887,  0.0809],\n",
       "         [-0.3569,  0.1546,  0.0753, -0.5843,  0.1570,  0.3657]]],\n",
       "       grad_fn=<ViewBackward0>)"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_sentences = torch.tensor(\n",
    "    [\n",
    "        [101, 3, 2, 5, 7, 8, 102, 0],\n",
    "        [101, 13, 8, 2, 9, 102, 0, 0],\n",
    "        [101, 21, 8, 15, 9, 7, 13, 102],\n",
    "    ]\n",
    ")\n",
    "\n",
    "src_max_len = 512\n",
    "src_vocab_size = 200\n",
    "d_model = 6\n",
    "src_heads = 2\n",
    "embedding_cls = Embedding(\n",
    "    vocab_size=src_vocab_size, max_len=src_max_len, d_model=d_model\n",
    ")\n",
    "embedding_results = embedding_cls(batch_sentences)\n",
    "\n",
    "\n",
    "mask = Mask()\n",
    "src_mask = mask.padding_mask(batch_sentences)\n",
    "\n",
    "attention_cls = MultiHeadAttention(src_heads, d_model)\n",
    "attention_cls(embedding_results, embedding_results, embedding_results, src_mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LayerNorm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "class LayerNorm(nn.Module):\n",
    "    def __init__(self, d_model: int, eps: float = 1e-6):\n",
    "        super().__init__()\n",
    "        self.gama = nn.Parameter(torch.ones(d_model))\n",
    "        self.beta = nn.Parameter(torch.zeros(d_model))\n",
    "        self.eps = eps\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        \"\"\"x.shape=(batch_size,seq_len,d_model)\"\"\"\n",
    "        mean = x.mean(dim=-1, keepdim=True)\n",
    "        var = x.var(dim=-1, keepdim=True)\n",
    "        return (x - mean) / (torch.sqrt(var + self.eps)) * self.gama + self.beta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.9220,  0.3081, -0.5607,  0.8963, -1.0646,  1.3430],\n",
       "         [ 0.0922,  0.7240, -0.3764,  0.1275, -1.7239,  1.1566],\n",
       "         [-0.7673,  0.4614, -0.5221,  0.6954, -1.2439,  1.3765],\n",
       "         [-0.6938,  0.4627, -0.5116,  0.6840, -1.3073,  1.3660],\n",
       "         [-0.3658,  0.5307, -0.5158,  0.5744, -1.5195,  1.2961],\n",
       "         [-0.8288,  0.3166, -0.5720,  0.9063, -1.1456,  1.3235],\n",
       "         [-0.9503,  0.3180, -0.5472,  0.8776, -1.0492,  1.3511],\n",
       "         [-0.6751,  0.5215, -0.4606,  0.5788, -1.3470,  1.3823]],\n",
       "\n",
       "        [[-0.9624,  0.2757, -0.5563,  0.9527, -1.0250,  1.3153],\n",
       "         [-0.9486,  0.2419, -0.6616,  1.0943, -0.9513,  1.2254],\n",
       "         [-0.9434,  0.2421, -0.6543,  1.0843, -0.9622,  1.2335],\n",
       "         [-0.9538,  0.2862, -0.6569,  1.0001, -0.9612,  1.2856],\n",
       "         [-0.9726,  0.1985, -0.6677,  1.1343, -0.9033,  1.2108],\n",
       "         [-0.9713,  0.2723, -0.5699,  0.9756, -1.0072,  1.3005],\n",
       "         [-0.9408,  0.2992, -0.5944,  0.9555, -1.0258,  1.3064],\n",
       "         [-0.9344,  0.3052, -0.6067,  0.9594, -1.0251,  1.3016]],\n",
       "\n",
       "        [[-1.0668,  0.1455, -0.7396,  1.2307, -0.7032,  1.1334],\n",
       "         [-1.1689,  0.0124, -0.8205,  1.3880, -0.3613,  0.9503],\n",
       "         [-1.0792,  0.1270, -0.7706,  1.2825, -0.6412,  1.0814],\n",
       "         [-0.7874,  0.3860, -0.6683,  0.9641, -1.1408,  1.2464],\n",
       "         [-1.0960,  0.0768, -0.7705,  1.3391, -0.5822,  1.0329],\n",
       "         [-0.9182,  0.2092, -0.7327,  1.1903, -0.9058,  1.1571],\n",
       "         [-1.1045,  0.1094, -0.7728,  1.2946, -0.5948,  1.0682],\n",
       "         [-1.1174,  0.1482, -0.7437,  1.2255, -0.6383,  1.1256]]],\n",
       "       grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_sentences = torch.tensor(\n",
    "    [\n",
    "        [101, 3, 2, 5, 7, 8, 102, 0],\n",
    "        [101, 13, 8, 2, 9, 102, 0, 0],\n",
    "        [101, 21, 8, 15, 9, 7, 13, 102],\n",
    "    ]\n",
    ")\n",
    "\n",
    "src_max_len = 512\n",
    "src_vocab_size = 200\n",
    "d_model = 6\n",
    "src_heads = 2\n",
    "embedding_cls = Embedding(\n",
    "    vocab_size=src_vocab_size, max_len=src_max_len, d_model=d_model\n",
    ")\n",
    "embedding_results = embedding_cls(batch_sentences)\n",
    "\n",
    "\n",
    "mask = Mask()\n",
    "src_mask = mask.padding_mask(batch_sentences)\n",
    "\n",
    "attention_cls = MultiHeadAttention(src_heads, d_model)\n",
    "attention_cls_output = attention_cls(\n",
    "    embedding_results, embedding_results, embedding_results, src_mask\n",
    ")\n",
    "\n",
    "layer_norm_cls = LayerNorm(d_model)\n",
    "layer_norm_cls(attention_cls_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FeedForward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, d_model: int, hidden_dim: int):\n",
    "        super().__init__()\n",
    "        self.linear1 = nn.Linear(d_model, hidden_dim)\n",
    "        self.linear2 = nn.Linear(hidden_dim, d_model)\n",
    "        self.d_model = d_model\n",
    "        self.hidden_dim = hidden_dim\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        return self.linear2(torch.relu(self.linear1(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.2970, -0.2642, -0.1333,  0.1346, -0.0407, -0.0441],\n",
       "         [ 0.2865, -0.2142, -0.1056,  0.0586, -0.0690,  0.0267],\n",
       "         [ 0.2345, -0.2826, -0.2156,  0.3108,  0.0380, -0.1706],\n",
       "         [ 0.0659, -0.1865, -0.2565,  0.2692,  0.0484, -0.2983],\n",
       "         [ 0.1875, -0.2069, -0.1105,  0.1082,  0.1196, -0.2013],\n",
       "         [ 0.2690, -0.2611, -0.1498,  0.1757,  0.0030, -0.1001],\n",
       "         [ 0.3037, -0.2385, -0.1153,  0.0829, -0.0438, -0.0102],\n",
       "         [ 0.2634, -0.1494, -0.1002,  0.0008, -0.0661,  0.0206]],\n",
       "\n",
       "        [[ 0.2481, -0.1859, -0.1566,  0.1825, -0.1329,  0.1252],\n",
       "         [ 0.4056, -0.0463, -0.2222, -0.0841, -0.1300, -0.0345],\n",
       "         [ 0.2658, -0.1077, -0.1882,  0.1676, -0.1329,  0.1681],\n",
       "         [ 0.0838, -0.1866, -0.3830,  0.5116,  0.0107, -0.0707],\n",
       "         [ 0.0306, -0.1580, -0.3514,  0.4308,  0.0221, -0.1619],\n",
       "         [ 0.2467, -0.2389, -0.1472,  0.1849, -0.1098,  0.0474],\n",
       "         [ 0.3705, -0.0655, -0.1710, -0.0632, -0.1329, -0.0197],\n",
       "         [ 0.3861, -0.0630, -0.1781, -0.0601, -0.1479, -0.0069]],\n",
       "\n",
       "        [[ 0.2290, -0.2097, -0.1726,  0.2732, -0.1681,  0.1625],\n",
       "         [ 0.2099, -0.2006, -0.2385,  0.3856, -0.1689,  0.1899],\n",
       "         [ 0.2564, -0.1584, -0.2216,  0.3085, -0.1546,  0.2193],\n",
       "         [ 0.1976, -0.1250, -0.2943,  0.3671, -0.1052,  0.1793],\n",
       "         [-0.0072, -0.1258, -0.3554,  0.4146,  0.0081, -0.1111],\n",
       "         [ 0.1357, -0.1366, -0.0180,  0.0526,  0.2035, -0.2288],\n",
       "         [ 0.3768, -0.0608, -0.2223, -0.0491, -0.1409, -0.0098],\n",
       "         [ 0.2771, -0.2003, -0.1338,  0.1576, -0.1494,  0.1475]]],\n",
       "       grad_fn=<ViewBackward0>)"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_sentences = torch.tensor(\n",
    "    [\n",
    "        [101, 3, 2, 5, 7, 8, 102, 0],\n",
    "        [101, 13, 8, 2, 9, 102, 0, 0],\n",
    "        [101, 21, 8, 15, 9, 7, 13, 102],\n",
    "    ]\n",
    ")\n",
    "\n",
    "src_max_len = 512\n",
    "src_vocab_size = 200\n",
    "d_model = 6\n",
    "hidden_dim = 24\n",
    "src_heads = 2\n",
    "embedding_cls = Embedding(\n",
    "    vocab_size=src_vocab_size, max_len=src_max_len, d_model=d_model\n",
    ")\n",
    "embedding_results = embedding_cls(batch_sentences)\n",
    "\n",
    "\n",
    "mask = Mask()\n",
    "src_mask = mask.padding_mask(batch_sentences)\n",
    "\n",
    "attention_cls = MultiHeadAttention(src_heads, d_model)\n",
    "attention_cls_output = attention_cls(\n",
    "    embedding_results, embedding_results, embedding_results, src_mask\n",
    ")\n",
    "\n",
    "layer_norm_cls = LayerNorm(d_model)\n",
    "layer_norm_output = layer_norm_cls(attention_cls_output)\n",
    "\n",
    "feedforward_cls = FeedForward(d_model, hidden_dim)\n",
    "feedforward_cls(layer_norm_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 编码器层"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "class EncoderLayer(nn.Module):\n",
    "    def __init__(self, d_model: int, hidden_dim: int, heads: int):\n",
    "        super().__init__()\n",
    "        self.multi_head_atten = MultiHeadAttention(heads, d_model)\n",
    "        self.layer_norm = clone(LayerNorm(d_model), 2)\n",
    "        self.feedforward = FeedForward(d_model, hidden_dim)\n",
    "\n",
    "    def forward(self, x: torch.Tensor, mask=None):\n",
    "        atten_outputs = self.multi_head_atten(x, x, x, mask)\n",
    "        x += self.layer_norm[0](atten_outputs)\n",
    "        feedforward_outputs = self.feedforward(x)\n",
    "        x += self.layer_norm[1](feedforward_outputs)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 2.7471, -0.5135,  2.5910, -2.4158,  1.9173,  1.9910],\n",
       "         [ 1.1906,  0.9480,  3.0668, -0.0510, -0.3191,  1.4701],\n",
       "         [ 2.3394, -2.0119,  3.5881,  2.0119, -1.8817, -0.7396],\n",
       "         [ 1.9286, -1.9994,  2.3473,  2.6664, -3.2357, -1.0885],\n",
       "         [ 1.1343, -3.0320,  0.3227,  0.8000, -0.6706, -1.6313],\n",
       "         [-0.4487, -1.5924,  0.3783,  2.8060, -2.5042, -1.9292],\n",
       "         [ 0.7252,  3.0834,  0.6972,  1.5750,  0.1807, -0.7911],\n",
       "         [ 1.4904,  1.9779,  0.5275, -1.0639,  0.4326, -1.0736]],\n",
       "\n",
       "        [[ 2.2483,  0.2041,  2.0298, -2.3571,  1.9032,  2.2888],\n",
       "         [ 2.4459, -2.1319,  0.4876,  3.9433, -2.5750, -1.3785],\n",
       "         [ 1.7332, -2.0767,  0.0889,  2.6249, -2.6966, -1.9166],\n",
       "         [ 2.0567, -2.5172,  2.6904,  3.0931, -2.1513, -1.1646],\n",
       "         [ 0.8407, -2.6807,  1.2238,  3.7096, -2.4359, -0.4528],\n",
       "         [-0.4456,  1.0478,  1.7615,  3.1917, -1.1143, -0.3622],\n",
       "         [ 0.4674,  2.0756,  0.3029, -0.2607,  0.1678, -1.2248],\n",
       "         [ 1.4184,  2.0140,  0.5919, -0.9722,  0.3206, -1.0817]],\n",
       "\n",
       "        [[ 3.6704, -0.9324,  2.9985, -1.3582,  0.9709,  0.9680],\n",
       "         [ 1.7600, -1.4482,  1.4905,  3.3013, -2.4922, -1.4171],\n",
       "         [ 1.9009, -2.0251,  0.0133,  2.5297, -2.6550, -2.0068],\n",
       "         [ 3.1529, -2.9995,  3.8228,  1.5415, -2.1029, -0.2650],\n",
       "         [ 0.8912, -2.6451,  1.2833,  3.5705, -2.3836, -0.5116],\n",
       "         [ 1.1064, -1.8252, -0.1361,  1.5054, -1.2178, -1.7364],\n",
       "         [ 1.4804, -1.8737,  0.6016,  3.9177, -2.5923, -1.2415],\n",
       "         [ 1.5727,  3.0175,  0.9567,  1.3380,  0.0741, -0.7259]]],\n",
       "       grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_sentences = torch.tensor(\n",
    "    [\n",
    "        [101, 3, 2, 5, 7, 8, 102, 0],\n",
    "        [101, 13, 8, 2, 9, 102, 0, 0],\n",
    "        [101, 21, 8, 15, 9, 7, 13, 102],\n",
    "    ]\n",
    ")\n",
    "\n",
    "src_max_len = 512\n",
    "src_vocab_size = 200\n",
    "d_model = 6\n",
    "hidden_dim = 24\n",
    "src_heads = 2\n",
    "embedding_cls = Embedding(\n",
    "    vocab_size=src_vocab_size, max_len=src_max_len, d_model=d_model\n",
    ")\n",
    "embedding_results = embedding_cls(batch_sentences)\n",
    "\n",
    "\n",
    "mask = Mask()\n",
    "src_mask = mask.padding_mask(batch_sentences)\n",
    "\n",
    "encoder_layer = EncoderLayer(d_model, hidden_dim, src_heads)\n",
    "encoder_layer(embedding_results, src_mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, layer: EncoderLayer, nums: int):\n",
    "        super().__init__()\n",
    "        self.layers = clone(layer, nums)\n",
    "\n",
    "    def forward(self, x: torch.Tensor, mask=None):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, mask)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ -4.6929, -14.1930,   8.5558,  13.6445,  -7.7855,   6.1300],\n",
       "         [ -9.1643, -14.2703,  11.4374,  13.1600,  -2.4385,   4.1526],\n",
       "         [ -9.6081, -14.7413,  10.7707,  13.3700,  -6.8403,   8.1122],\n",
       "         [ -7.3210, -15.1406,   8.8960,  12.9493,  -7.5348,   9.9626],\n",
       "         [ -7.4417, -12.3003,   8.1422,  10.1414,  -8.1089,   7.5740],\n",
       "         [-19.6019, -14.0036,  16.9428,  11.4761,   1.7704,   5.4786],\n",
       "         [-16.9150, -13.0332,  17.0304,  12.7865,   2.8235,   4.9336],\n",
       "         [ -8.6086, -13.7271,  13.2326,  14.8531,  -1.7082,   3.7085]],\n",
       "\n",
       "        [[ -4.6210, -14.7225,   8.3883,  13.6017,  -6.7144,   5.7269],\n",
       "         [ -9.5332, -13.4827,   7.9000,  10.6679,  -1.5611,   8.5812],\n",
       "         [-13.6171, -16.5595,  14.8419,  12.9995,   1.3102,   4.1346],\n",
       "         [ -9.6144, -15.8478,  10.7009,  13.0335,  -7.8042,   9.2960],\n",
       "         [ -8.4473, -14.8369,  11.9023,  14.1387,  -6.4568,   6.5284],\n",
       "         [-16.9718, -14.2307,  16.6397,  12.4862,   2.5405,   5.7704],\n",
       "         [-11.5194, -13.8120,  13.9086,  13.2505,  -0.5534,   5.7133],\n",
       "         [ -9.6714, -13.5965,  13.4857,  13.8347,  -0.7640,   4.4618]],\n",
       "\n",
       "        [[ -5.2431, -13.7057,   9.0487,  13.1895,  -7.9259,   6.2955],\n",
       "         [ -8.5428, -15.7068,   9.7700,  13.1180,  -7.3126,  10.1325],\n",
       "         [-13.4682, -16.7263,  14.6673,  12.8701,   1.7208,   4.0459],\n",
       "         [-10.8998, -16.2200,   9.3432,  11.6446,  -1.8805,   8.6230],\n",
       "         [-10.3338, -15.0628,  12.6428,  13.3485,  -4.3458,   6.5796],\n",
       "         [  6.6034,   0.5667,  -9.4518,  -0.2943,   6.3816,  -5.0259],\n",
       "         [-14.4816,  -7.5138,   8.5045,   4.6892,   3.2112,   7.6633],\n",
       "         [-14.1348, -14.0967,  15.7940,  13.1367,   3.2259,   4.4634]]],\n",
       "       grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_sentences = torch.tensor(\n",
    "    [\n",
    "        [101, 3, 2, 5, 7, 8, 102, 0],\n",
    "        [101, 13, 8, 2, 9, 102, 0, 0],\n",
    "        [101, 21, 8, 15, 9, 7, 13, 102],\n",
    "    ]\n",
    ")\n",
    "\n",
    "src_max_len = 512\n",
    "src_vocab_size = 200\n",
    "d_model = 6\n",
    "hidden_dim = 24\n",
    "src_heads = 2\n",
    "layer_nums = 8\n",
    "embedding_cls = Embedding(\n",
    "    vocab_size=src_vocab_size, max_len=src_max_len, d_model=d_model\n",
    ")\n",
    "embedding_results = embedding_cls(batch_sentences)\n",
    "\n",
    "\n",
    "mask = Mask()\n",
    "src_mask = mask.padding_mask(batch_sentences)\n",
    "\n",
    "encoder_layer = EncoderLayer(d_model, hidden_dim, src_heads)\n",
    "encoder = Encoder(encoder_layer, layer_nums)\n",
    "encoder(embedding_results, src_mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 解码器"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "class DecoderLayer(nn.Module):\n",
    "    def __init__(self, d_model: int, hidden_dim: int, heads: int):\n",
    "        super().__init__()\n",
    "        self.multi_head_attens = clone(MultiHeadAttention(heads, d_model), 2)\n",
    "        self.layer_norms = clone(LayerNorm(d_model), 3)\n",
    "        self.feedforward = FeedForward(d_model, hidden_dim)\n",
    "\n",
    "    def forward(\n",
    "        self, memory: torch.Tensor, x: torch.Tensor, tgt_mask=None, src_mask=None\n",
    "    ):\n",
    "        masked_atten_outputs = self.multi_head_attens[0](x, x, x, tgt_mask)\n",
    "        x += self.layer_norms[0](masked_atten_outputs)\n",
    "        multi_atten_outputs = self.multi_head_attens[1](x, memory, memory, src_mask)\n",
    "        x += self.layer_norms[1](multi_atten_outputs)\n",
    "        feed_forward_outputs = self.feedforward(x)\n",
    "        x += self.layer_norms[2](feed_forward_outputs)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.2220,  0.7314, -3.4175,  0.6165,  0.4795,  0.9315],\n",
       "         [ 0.5121,  1.1963, -3.1803,  0.6147,  3.7034, -0.6640],\n",
       "         [ 0.2788,  2.0792, -3.8357,  0.9190,  2.3652,  0.5567],\n",
       "         [-1.8392,  2.5938, -2.4730,  0.4890,  2.3009,  0.9286],\n",
       "         [-0.7027,  1.3716, -1.7195,  1.6228,  2.4574,  1.1560],\n",
       "         [-2.0800,  1.4047,  0.6848,  0.3811,  3.7724,  2.4783],\n",
       "         [-1.3923,  0.5486, -1.3639,  1.8846,  2.0690,  1.3879],\n",
       "         [-0.4787,  2.1709, -1.7801,  2.5245,  1.9235,  0.5721]],\n",
       "\n",
       "        [[-0.1645,  0.9530, -3.6034,  0.6343,  0.5244,  0.7756],\n",
       "         [-0.6099,  3.3170, -2.5119, -0.3787,  1.6383, -0.2641],\n",
       "         [-0.2875,  2.3824, -2.3183, -0.4385,  1.0352, -0.5757],\n",
       "         [-0.0983,  2.9323, -0.8020,  1.7290,  2.6300,  0.7892],\n",
       "         [-0.8780,  1.0525, -2.5832, -0.1945,  1.9199, -0.4407],\n",
       "         [-2.1099, -0.1588, -1.7264,  2.0556,  2.1696,  1.5126],\n",
       "         [-1.5402,  2.3642, -1.9045,  2.7316,  1.9534,  0.5649],\n",
       "         [-0.5713,  2.3655, -1.9711,  2.6800,  1.9467,  0.4823]],\n",
       "\n",
       "        [[-0.2937,  1.1808, -3.5514,  0.8673,  0.3939,  0.5224],\n",
       "         [-1.2831,  3.2206, -0.1849,  1.7582, -0.7988, -0.6274],\n",
       "         [ 0.6080,  1.9068, -0.8751,  0.5089,  2.5852,  1.0108],\n",
       "         [-0.1430,  3.0736, -0.7725,  1.9602,  2.5281,  0.5338],\n",
       "         [-0.7059,  1.1177,  0.5637,  1.8850,  1.1550,  0.6286],\n",
       "         [-0.8177,  2.7997, -1.9044,  1.9492,  2.1507,  0.7812],\n",
       "         [-1.3044,  2.8334, -1.1329,  1.8603,  1.0622, -0.8541],\n",
       "         [-0.3645,  1.0389, -1.8139,  2.1569,  1.8282,  1.0512]]],\n",
       "       grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_sentences = torch.tensor(\n",
    "    [\n",
    "        [101, 3, 2, 5, 7, 8, 102, 0],\n",
    "        [101, 13, 8, 2, 9, 102, 0, 0],\n",
    "        [101, 21, 8, 15, 9, 7, 13, 102],\n",
    "    ]\n",
    ")\n",
    "decoder_inputs = torch.tensor(\n",
    "    [\n",
    "        [101, 13, 6, 9, 2, 7, 102, 0],\n",
    "        [101, 10, 18, 5, 3, 102, 0, 0],\n",
    "        [101, 1, 4, 5, 19, 2, 23, 102],\n",
    "    ]\n",
    ")\n",
    "\n",
    "src_max_len = 512\n",
    "src_vocab_size = 200\n",
    "d_model = 6\n",
    "hidden_dim = 24\n",
    "src_heads = 2\n",
    "layer_nums = 8\n",
    "src_embedding_cls = Embedding(\n",
    "    vocab_size=src_vocab_size, max_len=src_max_len, d_model=d_model\n",
    ")\n",
    "src_embedding_results = src_embedding_cls(batch_sentences)\n",
    "tgt_embedding_cls = Embedding(\n",
    "    vocab_size=src_vocab_size, max_len=src_max_len, d_model=d_model\n",
    ")\n",
    "tgt_embedding_results = tgt_embedding_cls(decoder_inputs)\n",
    "\n",
    "\n",
    "mask = Mask()\n",
    "src_mask = mask.padding_mask(batch_sentences)\n",
    "\n",
    "encoder_layer = EncoderLayer(d_model, hidden_dim, src_heads)\n",
    "encoder = Encoder(encoder_layer, layer_nums)\n",
    "encoder_outputs = encoder(src_embedding_results, src_mask)\n",
    "\n",
    "decoder_layer_cls = DecoderLayer(d_model, hidden_dim, src_heads)\n",
    "decoder_layer_cls(encoder_outputs, tgt_embedding_results, src_mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 解码器"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, layer: DecoderLayer, nums: int):\n",
    "        super().__init__()\n",
    "        self.layers = clone(layer, nums)\n",
    "\n",
    "    def forward(\n",
    "        self, memory: torch.Tensor, x: torch.Tensor, tgt_mask=None, src_mask=None\n",
    "    ):\n",
    "        for layer in self.layers:\n",
    "            x = layer(memory, x, tgt_mask, src_mask)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 21.2602,   9.2331, -24.4601,  -1.1073, -13.6907,  13.5291],\n",
       "         [ 22.5556,   8.9649, -23.6728,  -3.4604, -18.6228,  10.3777],\n",
       "         [ 23.7073,   8.2565, -21.5613,  -2.2190, -16.2544,  10.4904],\n",
       "         [ 18.9561,  11.7710, -24.0340,  -1.1761, -13.7383,  11.1137],\n",
       "         [ 21.6701,  10.4643, -24.0852,  -2.1130, -15.9563,  10.4645],\n",
       "         [ 21.3494,   8.6791, -23.3579,  -0.9561, -12.1208,  12.1037],\n",
       "         [ 22.1133,  10.8962, -23.5492,  -1.7947, -15.1828,  10.3826],\n",
       "         [ 21.8016,   8.5705, -22.9546,  -2.4148, -13.9840,  12.1114]],\n",
       "\n",
       "        [[ 22.5688,   6.3563, -22.4941,  -0.2357, -13.8495,  12.4185],\n",
       "         [ 21.1378,   5.2267, -23.8733,  -0.1486, -12.6912,  14.1384],\n",
       "         [ 21.7826,   6.0262, -21.8250,  -0.9273, -14.1055,  12.4486],\n",
       "         [ 22.0722,   8.3679, -20.9050,  -1.2527, -15.2012,   9.1900],\n",
       "         [ 22.8378,  10.7941, -19.0810,  -1.0239, -20.1041,   7.5475],\n",
       "         [ 23.6377,   7.9679, -20.6503,  -1.2229, -16.4547,   8.1964],\n",
       "         [ 23.6782,   6.5679, -20.2552,  -1.6901, -15.4150,   9.4815],\n",
       "         [ 23.5298,   5.4601, -20.7085,  -1.4546, -14.4153,  10.7187]],\n",
       "\n",
       "        [[ 20.1305,  10.0453, -21.7415,   1.8975, -15.6088,  10.0413],\n",
       "         [ 22.3735,   9.0709, -19.9949,   0.8463, -17.1236,   7.4608],\n",
       "         [ 20.4859,   7.0959, -21.4604,  -0.8791, -17.5626,  10.3777],\n",
       "         [ 19.2791,  11.0129, -21.0347,   0.4977, -15.6252,   8.1413],\n",
       "         [ 18.9553,   9.4070, -20.5128,  -0.2352, -15.5888,   8.6154],\n",
       "         [ 21.0967,  11.3241, -21.6414,   0.7473, -17.5970,   7.2878],\n",
       "         [ 20.7534,  10.6684, -22.6021,   1.4792, -15.8171,   9.7833],\n",
       "         [ 21.6115,   9.6864, -20.7119,   1.8062, -16.0208,   7.2568]]],\n",
       "       grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_sentences = torch.tensor(\n",
    "    [\n",
    "        [101, 3, 2, 5, 7, 8, 102, 0],\n",
    "        [101, 13, 8, 2, 9, 102, 0, 0],\n",
    "        [101, 21, 8, 15, 9, 7, 13, 102],\n",
    "    ]\n",
    ")\n",
    "decoder_inputs = torch.tensor(\n",
    "    [\n",
    "        [101, 13, 6, 9, 2, 7, 102, 0],\n",
    "        [101, 10, 18, 5, 3, 102, 0, 0],\n",
    "        [101, 1, 4, 5, 19, 2, 23, 102],\n",
    "    ]\n",
    ")\n",
    "\n",
    "src_max_len = 512\n",
    "src_vocab_size = 200\n",
    "d_model = 6\n",
    "hidden_dim = 24\n",
    "src_heads = 2\n",
    "layer_nums = 8\n",
    "src_embedding_cls = Embedding(\n",
    "    vocab_size=src_vocab_size, max_len=src_max_len, d_model=d_model\n",
    ")\n",
    "src_embedding_results = src_embedding_cls(batch_sentences)\n",
    "tgt_embedding_cls = Embedding(\n",
    "    vocab_size=src_vocab_size, max_len=src_max_len, d_model=d_model\n",
    ")\n",
    "tgt_embedding_results = tgt_embedding_cls(decoder_inputs)\n",
    "\n",
    "\n",
    "mask = Mask()\n",
    "src_mask = mask.padding_mask(batch_sentences)\n",
    "\n",
    "encoder_layer = EncoderLayer(d_model, hidden_dim, src_heads)\n",
    "encoder = Encoder(encoder_layer, layer_nums)\n",
    "encoder_outputs = encoder(src_embedding_results, src_mask)\n",
    "\n",
    "decoder_layer_cls = DecoderLayer(d_model, hidden_dim, src_heads)\n",
    "decoder = Decoder(decoder_layer_cls, layer_nums)\n",
    "decoder(encoder_outputs, tgt_embedding_results, src_mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 生成器"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, vocab_size: int, d_model: int):\n",
    "        super().__init__()\n",
    "        self.generator = nn.Linear(d_model, vocab_size)\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        \"\"\"x.shape=(batch_size,seq_len,d_model)\"\"\"\n",
    "        return F.log_softmax(self.generator(x), dim=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transormer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "class Transformer(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        src_vocab_size: int,\n",
    "        tgt_vocab_size: int,\n",
    "        src_d_model: int,\n",
    "        tgt_d_model: int,\n",
    "        src_hidden_dim: int,\n",
    "        tgt_hidden_dim: int,\n",
    "        src_heads: int,\n",
    "        tgt_heads: int,\n",
    "        layer_nums: int,\n",
    "        max_len: int,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.encoder_layer = EncoderLayer(src_d_model, src_hidden_dim, src_heads)\n",
    "        self.decoder_layer = DecoderLayer(tgt_d_model, tgt_hidden_dim, tgt_heads)\n",
    "        self.encoder = Encoder(self.encoder_layer, layer_nums)\n",
    "        self.decoder = Decoder(self.decoder_layer, layer_nums)\n",
    "        self.src_embedding = Embedding(src_vocab_size, max_len, src_d_model)\n",
    "        self.tgt_embedding = Embedding(tgt_vocab_size, max_len, tgt_d_model)\n",
    "        self.generator = Generator(tgt_vocab_size, tgt_d_model)\n",
    "\n",
    "    def forward(self, src, tgt, src_mask=None, tgt_mask=None):\n",
    "        \"\"\"\n",
    "        src.shape=(batch_size,src_seq_len)\n",
    "        tgt.shape=(batch_size,tgt_seq_len)\n",
    "        \"\"\"\n",
    "        src_embed = self.src_embedding(src)\n",
    "        tgt_embed = self.tgt_embedding(tgt)\n",
    "        encoder_outputs = self.encoder(src_embed, src_mask)\n",
    "        decoder_outputs = self.decoder(encoder_outputs, tgt_embed, tgt_mask, src_mask)\n",
    "        return self.generator(decoder_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-23.6987, -22.2254, -22.7746,  ..., -23.5941, -25.6780, -46.6218],\n",
       "         [-30.9086, -26.8903, -22.5864,  ..., -16.3116, -21.9060, -56.5316],\n",
       "         [-25.2848, -21.4719, -19.5253,  ..., -14.5516, -15.5766, -48.5730],\n",
       "         ...,\n",
       "         [-25.4831, -22.5566, -18.8937,  ..., -12.8312, -15.1014, -46.1652],\n",
       "         [-23.3707, -23.2196, -22.7867,  ..., -22.6124, -24.8479, -46.8203],\n",
       "         [-25.8424, -24.0414, -25.2619,  ..., -22.7277, -28.8318, -49.9659]],\n",
       "\n",
       "        [[-16.7712, -17.3099, -28.5644,  ..., -31.7214, -30.8977, -21.3963],\n",
       "         [-13.9675, -15.1343, -29.1793,  ..., -27.7112, -32.6018, -19.2178],\n",
       "         [-31.4915, -25.8899, -21.2553,  ..., -13.3160, -20.6630, -52.3194],\n",
       "         ...,\n",
       "         [-20.3351, -22.2516, -27.1599,  ..., -34.2610, -32.0830, -33.2498],\n",
       "         [-20.7398, -20.4035, -27.5340,  ..., -28.5940, -34.9267, -36.7387],\n",
       "         [-21.0911, -19.9814, -27.4198,  ..., -27.1833, -34.8969, -37.5490]],\n",
       "\n",
       "        [[-13.5771, -16.1949, -25.2118,  ..., -28.2430, -26.8797, -17.3253],\n",
       "         [-19.0580, -16.4384, -23.1102,  ..., -24.2418, -28.4127, -39.5155],\n",
       "         [-10.6702, -13.2012, -25.8766,  ..., -25.8216, -26.0496, -15.2796],\n",
       "         ...,\n",
       "         [-27.7383, -23.9105, -22.4318,  ..., -14.6540, -19.9136, -52.4555],\n",
       "         [-19.3992, -16.1129, -23.6985,  ..., -27.0419, -29.6090, -39.8449],\n",
       "         [-16.8891, -20.3445, -23.7898,  ..., -32.0034, -29.8081, -28.5189]]],\n",
       "       grad_fn=<LogSoftmaxBackward0>)"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_sentences = torch.tensor(\n",
    "    [\n",
    "        [101, 3, 2, 5, 7, 8, 102, 0],\n",
    "        [101, 13, 8, 2, 9, 102, 0, 0],\n",
    "        [101, 21, 8, 15, 9, 7, 13, 102],\n",
    "    ]\n",
    ")\n",
    "decoder_inputs = torch.tensor(\n",
    "    [\n",
    "        [101, 13, 6, 9, 2, 7, 102, 0],\n",
    "        [101, 10, 18, 5, 3, 102, 0, 0],\n",
    "        [101, 1, 4, 5, 19, 2, 23, 102],\n",
    "    ]\n",
    ")\n",
    "src_vocab_size = 200\n",
    "tgt_vocab_size = 240\n",
    "src_d_model = 8\n",
    "tgt_d_model = 8\n",
    "src_hidden_dim = 24\n",
    "tgt_hidden_dim = 24\n",
    "src_heads = 4\n",
    "tgt_heads = 4\n",
    "layer_nums = 8\n",
    "max_len = 512\n",
    "\n",
    "\n",
    "mask = Mask()\n",
    "src_mask = mask.padding_mask(batch_sentences)\n",
    "tgt_mask = mask.merge_mask(decoder_inputs, decoder_inputs.shape[-1])\n",
    "\n",
    "transformer = Transformer(\n",
    "    src_vocab_size,\n",
    "    tgt_vocab_size,\n",
    "    src_d_model,\n",
    "    tgt_d_model,\n",
    "    src_hidden_dim,\n",
    "    tgt_hidden_dim,\n",
    "    src_heads,\n",
    "    tgt_heads,\n",
    "    layer_nums,\n",
    "    max_len,\n",
    ")\n",
    "transformer(batch_sentences, decoder_inputs, tgt_mask, src_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch2.4",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
